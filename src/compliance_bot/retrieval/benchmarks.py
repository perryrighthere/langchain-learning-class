"""Week 3 retrieval benchmark utilities."""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from statistics import mean
from time import perf_counter

from compliance_bot.providers.provider_registry import (
    resolve_embedding_provider,
    resolve_rerank_provider,
)
from compliance_bot.retrieval.indexer import RetrievalIndex, build_retrieval_index, load_manifest
from compliance_bot.retrieval.retriever import run_retrieval
from compliance_bot.schemas.retrieval import (
    RetrievalBenchmarkCase,
    RetrievalBenchmarkReport,
    RetrievalBenchmarkResult,
)


def _reciprocal_rank(expected_doc_ids: list[str], ranked_doc_ids: list[str]) -> float:
    expected = set(expected_doc_ids)
    if not expected:
        return 1.0

    for index, doc_id in enumerate(ranked_doc_ids, start=1):
        if doc_id in expected:
            return 1.0 / index
    return 0.0


def _recall_at_k(expected_doc_ids: list[str], ranked_doc_ids: list[str]) -> float:
    expected = set(expected_doc_ids)
    if not expected:
        return 1.0

    hits = expected.intersection(ranked_doc_ids)
    return len(hits) / len(expected)


def _p95(values: list[float]) -> float:
    if not values:
        return 0.0

    ordered = sorted(values)
    rank = max(0, int(round(0.95 * len(ordered))) - 1)
    return ordered[min(rank, len(ordered) - 1)]


def run_retrieval_benchmarks(
    index: RetrievalIndex,
    *,
    cases: list[RetrievalBenchmarkCase],
    top_k: int = 4,
    recall_floor: float = 0.6,
    latency_ceiling_ms: float = 60.0,
    rerank_provider: object | None = None,
    embedding_provider: object | None = None,
) -> RetrievalBenchmarkReport:
    """Run recall/latency benchmark cases and return aggregate quality gate result."""

    if top_k <= 0:
        raise ValueError("top_k must be > 0")

    results: list[RetrievalBenchmarkResult] = []
    latencies: list[float] = []
    recalls: list[float] = []
    reciprocal_ranks: list[float] = []

    for case in cases:
        start = perf_counter()
        response = run_retrieval(
            index,
            question=case.question,
            filters=case.filters,
            top_k=top_k,
            rerank_provider=rerank_provider,  # type: ignore[arg-type]
            embedding_provider=embedding_provider,  # type: ignore[arg-type]
        )
        latency_ms = (perf_counter() - start) * 1000.0

        ranked_doc_ids = [chunk.doc_id for chunk in response.retrieved_chunks]
        recall = _recall_at_k(case.expected_doc_ids, ranked_doc_ids)
        reciprocal_rank = _reciprocal_rank(case.expected_doc_ids, ranked_doc_ids)

        results.append(
            RetrievalBenchmarkResult(
                case_id=case.case_id,
                recall_at_k=recall,
                reciprocal_rank=reciprocal_rank,
                latency_ms=latency_ms,
                decision=response.decision,
                top_doc_ids=ranked_doc_ids,
            )
        )

        latencies.append(latency_ms)
        recalls.append(recall)
        reciprocal_ranks.append(reciprocal_rank)

    avg_recall = mean(recalls) if recalls else 0.0
    avg_rr = mean(reciprocal_ranks) if reciprocal_ranks else 0.0
    p95_latency = _p95(latencies)
    meets_gate = avg_recall >= recall_floor and p95_latency <= latency_ceiling_ms

    return RetrievalBenchmarkReport(
        top_k=top_k,
        avg_recall_at_k=avg_recall,
        avg_reciprocal_rank=avg_rr,
        p95_latency_ms=p95_latency,
        recall_floor=recall_floor,
        latency_ceiling_ms=latency_ceiling_ms,
        meets_quality_gate=meets_gate,
        results=results,
    )


def load_benchmark_cases(path: Path) -> list[RetrievalBenchmarkCase]:
    """Load benchmark case definitions from JSON."""

    payload = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(payload, list):
        raise ValueError("benchmark cases JSON must be a list")
    return [RetrievalBenchmarkCase.model_validate(item) for item in payload]


def _build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Run Week 3 retrieval benchmarks")
    parser.add_argument(
        "--manifest-path",
        type=Path,
        required=True,
        help="Path to manifest JSON generated by Week 2 pipeline",
    )
    parser.add_argument(
        "--cases-path",
        type=Path,
        required=True,
        help="Path to benchmark case JSON file",
    )
    parser.add_argument("--top-k", type=int, default=1)
    parser.add_argument("--recall-floor", type=float, default=0.75)
    parser.add_argument("--latency-ceiling-ms", type=float, default=60.0)
    parser.add_argument(
        "--embedding-provider",
        choices=("auto", "none", "siliconflow"),
        default="auto",
        help="Embedding provider mode for practical retrieval scoring",
    )
    parser.add_argument(
        "--rerank-provider",
        choices=("auto", "none", "siliconflow"),
        default="auto",
        help="Rerank provider mode for practical retrieval scoring",
    )
    return parser


def main() -> None:
    """CLI entrypoint for Week 3 retrieval quality checks."""

    args = _build_parser().parse_args()
    embedding_provider = resolve_embedding_provider(args.embedding_provider)
    rerank_provider = resolve_rerank_provider(args.rerank_provider)

    manifest = load_manifest(args.manifest_path)
    index = build_retrieval_index(manifest, embedding_provider=embedding_provider)
    cases = load_benchmark_cases(args.cases_path)

    report = run_retrieval_benchmarks(
        index,
        cases=cases,
        top_k=args.top_k,
        recall_floor=args.recall_floor,
        latency_ceiling_ms=args.latency_ceiling_ms,
        embedding_provider=embedding_provider,
        rerank_provider=rerank_provider,
    )
    resolved_embedding_backend = (
        f"{embedding_provider.provider_name}:{embedding_provider.model}"
        if embedding_provider is not None
        else "lexical-only"
    )
    resolved_rerank_backend = (
        f"{rerank_provider.provider_name}:{rerank_provider.model}"
        if rerank_provider is not None
        else "none"
    )
    print(f"embedding_provider: {args.embedding_provider}")
    print(f"rerank_provider: {args.rerank_provider}")
    print(f"resolved_embedding_backend: {resolved_embedding_backend}")
    print(f"resolved_rerank_backend: {resolved_rerank_backend}")
    print(f"avg_recall_at_k: {report.avg_recall_at_k:.4f}")
    print(f"avg_reciprocal_rank: {report.avg_reciprocal_rank:.4f}")
    print(f"p95_latency_ms: {report.p95_latency_ms:.2f}")
    print(f"meets_quality_gate: {report.meets_quality_gate}")


if __name__ == "__main__":
    main()
